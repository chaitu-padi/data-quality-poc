{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4882cb4f",
   "metadata": {},
   "source": [
    "# Data Quality Rule Recommendation Engine - Usage Examples\n",
    "\n",
    "This notebook demonstrates how to use the Data Quality Rule Recommendation Engine to analyze datasets and generate intelligent data quality rules using LLM inference.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Installation\n",
    "2. Basic Usage\n",
    "3. Advanced Features\n",
    "4. Custom Rule Generation\n",
    "5. Testing and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8aa321",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's import the required libraries and initialize the recommendation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from backend.recommendation_engine import DataQualityRuleRecommendationEngine\n",
    "from backend.agents.rule_recommender import DataQualityRuleRecommender\n",
    "\n",
    "# Initialize the recommendation engine\n",
    "engine = DataQualityRuleRecommendationEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8621122",
   "metadata": {},
   "source": [
    "## 2. Basic Usage\n",
    "\n",
    "Let's load a sample dataset and generate basic data quality recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050fde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "df = pd.read_csv('sample_customer_data.csv')\n",
    "\n",
    "# Display basic dataset information\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in df.columns:\n",
    "    print(f\"- {col}: {df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d25bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "recommendations = engine.analyze_data_and_recommend_rules(\n",
    "    df=df,\n",
    "    technical_metadata=None,\n",
    "    data_lineage=None\n",
    ")\n",
    "\n",
    "# Display recommendations summary\n",
    "severity_counts = {\n",
    "    'CRITICAL': 0,\n",
    "    'HIGH': 0,\n",
    "    'MEDIUM': 0,\n",
    "    'LOW': 0\n",
    "}\n",
    "\n",
    "for rec in recommendations:\n",
    "    severity_counts[rec['severity']] += 1\n",
    "\n",
    "print(\"Recommendations Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total rules: {len(recommendations)}\")\n",
    "for severity, count in severity_counts.items():\n",
    "    print(f\"{severity}: {count} rules\")\n",
    "\n",
    "# Display detailed recommendations\n",
    "print(\"\\nDetailed Recommendations:\")\n",
    "print(\"-\" * 50)\n",
    "for i, rec in enumerate(recommendations[:5], 1):\n",
    "    print(f\"\\nRule {i}:\")\n",
    "    print(f\"Name: {rec['rule_name']}\")\n",
    "    print(f\"Type: {rec['rule_type']}\")\n",
    "    print(f\"Severity: {rec['severity']}\")\n",
    "    print(f\"Description: {rec['description']}\")\n",
    "    print(f\"SQL: {rec['sql_rule']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00581b13",
   "metadata": {},
   "source": [
    "## 3. Advanced Features\n",
    "\n",
    "Now let's explore some advanced features, including custom rule generation and pattern detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the rule recommender for advanced features\n",
    "recommender = DataQualityRuleRecommender()\n",
    "\n",
    "# Select a specific column for detailed analysis\n",
    "column_name = 'customer_email'  # Replace with an actual column name from your dataset\n",
    "\n",
    "# Load metadata files\n",
    "with open('business_glossary.json', 'r') as f:\n",
    "    business_glossary = json.load(f)\n",
    "    \n",
    "with open('technical_metadata.json', 'r') as f:\n",
    "    technical_metadata = json.load(f)\n",
    "    \n",
    "with open('data_lineage.json', 'r') as f:\n",
    "    data_lineage = json.load(f)\n",
    "\n",
    "# Generate custom rules for the column\n",
    "custom_rules = recommender.analyze_and_recommend(\n",
    "    df=df,\n",
    "    column_name=column_name,\n",
    "    business_glossary=business_glossary,\n",
    "    technical_metadata=technical_metadata,\n",
    "    data_lineage=data_lineage\n",
    ")\n",
    "\n",
    "# Display custom rules\n",
    "print(f\"Custom Rules for column '{column_name}':\")\n",
    "print(\"-\" * 50)\n",
    "for i, rule in enumerate(custom_rules, 1):\n",
    "    print(f\"\\nCustom Rule {i}:\")\n",
    "    print(f\"Name: {rule['rule_name']}\")\n",
    "    print(f\"Description: {rule['description']}\")\n",
    "    print(f\"SQL: {rule['sql_rule']}\")\n",
    "    if 'spark_sql' in rule:\n",
    "        print(f\"Spark SQL: {rule['spark_sql']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705c65e",
   "metadata": {},
   "source": [
    "## 4. Pattern Detection\n",
    "\n",
    "Let's analyze patterns in the data using the built-in pattern detection capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee27e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_column_patterns(df, column_name):\n",
    "    \"\"\"Analyze patterns in a specific column\"\"\"\n",
    "    series = df[column_name]\n",
    "    patterns = {}\n",
    "    \n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        # Numerical column analysis\n",
    "        q1 = series.quantile(0.25)\n",
    "        q3 = series.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "        \n",
    "        patterns['statistics'] = {\n",
    "            'mean': series.mean(),\n",
    "            'std': series.std(),\n",
    "            'min': series.min(),\n",
    "            'max': series.max(),\n",
    "            'median': series.median()\n",
    "        }\n",
    "        patterns['outliers'] = {\n",
    "            'count': len(outliers),\n",
    "            'percentage': (len(outliers) / len(series)) * 100,\n",
    "            'bounds': {'lower': float(lower_bound), 'upper': float(upper_bound)}\n",
    "        }\n",
    "    else:\n",
    "        # String column analysis\n",
    "        patterns['length_stats'] = {\n",
    "            'avg_length': series.str.len().mean(),\n",
    "            'max_length': series.str.len().max(),\n",
    "            'min_length': series.str.len().min()\n",
    "        }\n",
    "        patterns['unique_values'] = {\n",
    "            'count': series.nunique(),\n",
    "            'percentage': (series.nunique() / len(series)) * 100\n",
    "        }\n",
    "        patterns['top_values'] = series.value_counts().head(5).to_dict()\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Analyze patterns for a few columns\n",
    "for col in df.columns[:3]:  # Analyze first 3 columns\n",
    "    print(f\"\\nPattern Analysis for '{col}':\")\n",
    "    print(\"-\" * 50)\n",
    "    patterns = analyze_column_patterns(df, col)\n",
    "    print(json.dumps(patterns, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c7ba6",
   "metadata": {},
   "source": [
    "## 5. Testing and Validation\n",
    "\n",
    "Finally, let's validate the recommendations and test their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_rules(df, recommendations):\n",
    "    \"\"\"Validate rules against the dataset\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for rule in recommendations:\n",
    "        try:\n",
    "            # Convert SQL rule to a pandas query where possible\n",
    "            if 'IS NULL' in rule['sql_rule']:\n",
    "                violation_count = df[rule['column']].isnull().sum()\n",
    "            elif 'DISTINCT' in rule['sql_rule']:\n",
    "                violation_count = len(df) - df[rule['column']].nunique()\n",
    "            else:\n",
    "                # For more complex rules, just report them\n",
    "                violation_count = None\n",
    "            \n",
    "            results.append({\n",
    "                'rule_name': rule['rule_name'],\n",
    "                'severity': rule['severity'],\n",
    "                'violations': violation_count,\n",
    "                'status': 'Validated' if violation_count is not None else 'Complex Rule'\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'rule_name': rule['rule_name'],\n",
    "                'severity': rule['severity'],\n",
    "                'violations': None,\n",
    "                'status': f'Error: {str(e)}'\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Validate the recommendations\n",
    "validation_results = validate_rules(df, recommendations)\n",
    "\n",
    "# Display validation results\n",
    "print(\"Rule Validation Results:\")\n",
    "print(\"-\" * 50)\n",
    "for result in validation_results:\n",
    "    print(f\"\\nRule: {result['rule_name']}\")\n",
    "    print(f\"Severity: {result['severity']}\")\n",
    "    print(f\"Violations: {result['violations']}\")\n",
    "    print(f\"Status: {result['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01459b2b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the key features of the Data Quality Rule Recommendation Engine:\n",
    "1. Basic rule generation using LLM inference\n",
    "2. Custom rule creation based on data patterns\n",
    "3. Advanced pattern detection and analysis\n",
    "4. Rule validation and testing\n",
    "\n",
    "For more information, refer to the documentation in the repository."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
